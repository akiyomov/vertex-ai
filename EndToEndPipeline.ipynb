{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97a19546-ab21-4811-89e6-e4657bdc7d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fa2f25b-7265-4398-9ee2-5d9739a5d0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-aiplatform==1.0.0\n",
      "  Downloading google_cloud_aiplatform-1.0.0-py2.py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.22.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.0.0) (1.34.0)\n",
      "Requirement already satisfied: proto-plus>=1.10.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.0.0) (1.22.3)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.0.0) (23.1)\n",
      "Collecting google-cloud-storage<2.0.0dev,>=1.32.0 (from google-cloud-aiplatform==1.0.0)\n",
      "  Downloading google_cloud_storage-1.44.0-py2.py3-none-any.whl (106 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m106.8/106.8 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-cloud-bigquery<3.0.0dev,>=1.15.0 (from google-cloud-aiplatform==1.0.0)\n",
      "  Downloading google_cloud_bigquery-2.34.4-py2.py3-none-any.whl (206 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m206.6/206.6 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform==1.0.0) (1.60.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform==1.0.0) (3.20.3)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform==1.0.0) (2.23.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform==1.0.0) (2.31.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform==1.0.0) (1.58.0)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform==1.0.0) (1.48.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.0.0) (2.3.3)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.0.0) (2.6.0)\n",
      "Collecting packaging>=14.3 (from google-cloud-aiplatform==1.0.0)\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.0.0) (2.8.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage<2.0.0dev,>=1.32.0->google-cloud-aiplatform==1.0.0) (1.16.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=14.3->google-cloud-aiplatform==1.0.0) (3.1.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform==1.0.0) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform==1.0.0) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform==1.0.0) (4.9)\n",
      "Requirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform==1.0.0) (1.26.16)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.0.0) (1.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform==1.0.0) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform==1.0.0) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform==1.0.0) (2023.7.22)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform==1.0.0) (0.5.0)\n",
      "Installing collected packages: packaging, google-cloud-storage, google-cloud-bigquery, google-cloud-aiplatform\n",
      "\u001b[33m  WARNING: The script tb-gcp-uploader is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed google-cloud-aiplatform-1.0.0 google-cloud-bigquery-2.34.4 google-cloud-storage-1.44.0 packaging-21.3\n",
      "Collecting kfp\n",
      "  Downloading kfp-2.4.0.tar.gz (392 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m392.3/392.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting google-cloud-pipeline-components==0.1.1\n",
      "  Downloading google_cloud_pipeline_components-0.1.1-py3-none-any.whl (17 kB)\n",
      "Collecting kfp\n",
      "  Downloading kfp-1.8.22.tar.gz (304 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m304.9/304.9 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: google-cloud-aiplatform>=1.0.0 in ./.local/lib/python3.7/site-packages (from google-cloud-pipeline-components==0.1.1) (1.0.0)\n",
      "Collecting absl-py<2,>=0.9 (from kfp)\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: PyYAML<7,>=5.3 in /opt/conda/lib/python3.7/site-packages (from kfp) (6.0.1)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.7/site-packages (from kfp) (1.34.0)\n",
      "Requirement already satisfied: google-cloud-storage<3,>=1.20.0 in ./.local/lib/python3.7/site-packages (from kfp) (1.44.0)\n",
      "Collecting kubernetes<26,>=8.0.0 (from kfp)\n",
      "  Downloading kubernetes-25.3.0-py2.py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-api-python-client<2,>=1.7.8 in /opt/conda/lib/python3.7/site-packages (from kfp) (1.8.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from kfp) (2.23.0)\n",
      "Collecting requests-toolbelt<1,>=0.8.0 (from kfp)\n",
      "  Downloading requests_toolbelt-0.10.1-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cloudpickle<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from kfp) (2.2.1)\n",
      "Collecting kfp-server-api<2.0.0,>=1.1.2 (from kfp)\n",
      "  Downloading kfp-server-api-1.8.5.tar.gz (58 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.1/58.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: jsonschema<5,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp) (4.17.3)\n",
      "Collecting tabulate<1,>=0.8.6 (from kfp)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: click<9,>=7.1.2 in /opt/conda/lib/python3.7/site-packages (from kfp) (8.1.7)\n",
      "Requirement already satisfied: Deprecated<2,>=1.2.7 in /opt/conda/lib/python3.7/site-packages (from kfp) (1.2.14)\n",
      "Collecting strip-hints<1,>=0.1.8 (from kfp)\n",
      "  Downloading strip-hints-0.1.10.tar.gz (29 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting docstring-parser<1,>=0.7.3 (from kfp)\n",
      "  Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
      "Collecting kfp-pipeline-spec<0.2.0,>=0.1.16 (from kfp)\n",
      "  Downloading kfp_pipeline_spec-0.1.16-py3-none-any.whl (19 kB)\n",
      "Collecting fire<1,>=0.3.1 (from kfp)\n",
      "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: protobuf<4,>=3.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp) (3.20.3)\n",
      "Requirement already satisfied: uritemplate<4,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp) (3.0.1)\n",
      "Requirement already satisfied: urllib3<2 in /opt/conda/lib/python3.7/site-packages (from kfp) (1.26.16)\n",
      "Requirement already satisfied: pydantic<2,>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from kfp) (1.10.12)\n",
      "Requirement already satisfied: typer<1.0,>=0.3.2 in /opt/conda/lib/python3.7/site-packages (from kfp) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from kfp) (4.7.1)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click<9,>=7.1.2->kfp) (4.11.4)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated<2,>=1.2.7->kfp) (1.15.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from fire<1,>=0.3.1->kfp) (1.16.0)\n",
      "Collecting termcolor (from fire<1,>=0.3.1->kfp)\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (1.60.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (2.31.0)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp) (0.1.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.1->kfp) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.1->kfp) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.1->kfp) (4.9)\n",
      "Requirement already satisfied: proto-plus>=1.10.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform>=1.0.0->google-cloud-pipeline-components==0.1.1) (1.22.3)\n",
      "Requirement already satisfied: packaging>=14.3 in ./.local/lib/python3.7/site-packages (from google-cloud-aiplatform>=1.0.0->google-cloud-pipeline-components==0.1.1) (21.3)\n",
      "Requirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in ./.local/lib/python3.7/site-packages (from google-cloud-aiplatform>=1.0.0->google-cloud-pipeline-components==0.1.1) (2.34.4)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage<3,>=1.20.0->kfp) (2.3.3)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage<3,>=1.20.0->kfp) (2.6.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<5,>=3.0.1->kfp) (23.1.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<5,>=3.0.1->kfp) (5.12.0)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in /opt/conda/lib/python3.7/site-packages (from jsonschema<5,>=3.0.1->kfp) (1.3.10)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<5,>=3.0.1->kfp) (0.18.1)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (2.8.2)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<26,>=8.0.0->kfp) (68.2.2)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<26,>=8.0.0->kfp) (1.6.1)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<26,>=8.0.0->kfp) (1.3.1)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints<1,>=0.1.8->kfp) (0.41.2)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (1.58.0)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (1.48.2)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage<3,>=1.20.0->kfp) (1.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from httplib2<1dev,>=0.9.2->google-api-python-client<2,>=1.7.8->kfp) (3.1.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.7/site-packages (from importlib-resources>=1.4.0->jsonschema<5,>=3.0.1->kfp) (3.15.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.1->kfp) (0.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (3.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<26,>=8.0.0->kfp) (3.2.2)\n",
      "Building wheels for collected packages: kfp, fire, kfp-server-api, strip-hints\n",
      "  Building wheel for kfp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kfp: filename=kfp-1.8.22-py3-none-any.whl size=426966 sha256=d41a9a170518926e1469fb2fdd860fc18e85b1c3e5089cf308911a8d8d90e7b5\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/f1/ea/2f/99e16eb441be7831b69cb568df54a0dd04ab31cb758bfc6819\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116934 sha256=f80685b07fdd259b196786ccdefcc36d2d75bdda4b83bb1d37ab5ade3ed36776\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/20/97/e1/dd2c472bebcdcaa85fdc07d0f19020299f1c86773028860c53\n",
      "  Building wheel for kfp-server-api (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kfp-server-api: filename=kfp_server_api-1.8.5-py3-none-any.whl size=99696 sha256=62cd0f0d4e640127438c39c96053db2a6833052fa4d276c5d9c4e44ca5415542\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/77/0e/7b/ed385d69453b7b754834c01d83fa9f5708ba66b4f6ed5d6a35\n",
      "  Building wheel for strip-hints (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for strip-hints: filename=strip_hints-0.1.10-py2.py3-none-any.whl size=22283 sha256=e5d567f71265660cc050eba94930cf4cd0b58e693acebd5f5fa27fe2a5cd591f\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/5e/14/c3/6e44e9b2545f2d570b03f5b6d38c00b7534aa8abb376978363\n",
      "Successfully built kfp fire kfp-server-api strip-hints\n",
      "Installing collected packages: termcolor, tabulate, strip-hints, kfp-pipeline-spec, docstring-parser, absl-py, requests-toolbelt, kfp-server-api, fire, kubernetes, kfp, google-cloud-pipeline-components\n",
      "\u001b[33m  WARNING: The script tabulate is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script strip-hints is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts dsl-compile, dsl-compile-v2 and kfp are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed absl-py-1.4.0 docstring-parser-0.15 fire-0.5.0 google-cloud-pipeline-components-0.1.1 kfp-1.8.22 kfp-pipeline-spec-0.1.16 kfp-server-api-1.8.5 kubernetes-25.3.0 requests-toolbelt-0.10.1 strip-hints-0.1.10 tabulate-0.9.0 termcolor-2.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install {USER_FLAG} google-cloud-aiplatform==1.0.0 --upgrade\n",
    "!pip3 install {USER_FLAG} kfp google-cloud-pipeline-components==0.1.1 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "135c62d3-15ca-4420-9dc2-4e198676ccbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1de0c40-6502-40d9-bd16-408b0b1eeee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 1.8.22\n",
      "google_cloud_pipeline_components version: 0.1.1\n"
     ]
    }
   ],
   "source": [
    "!python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "!python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c2886ed-7220-45b1-b876-a9d7e621a307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  qwiklabs-gcp-01-8b55c97e18cf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "PROJECT_ID = \"qwiklabs-gcp-01-8b55c97e18cf\"\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64ab8f25-4a28-4296-a9ca-b34dc851c6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME=\"gs://\" + PROJECT_ID + \"-bucket\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93342143-0d1c-4462-a772-00771cfaf2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, ClassificationMetrics, Metrics, component)\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c016521f-9129-4bbc-ba8d-029807f0bdb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/jupyter/.local/bin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'gs://qwiklabs-gcp-01-8b55c97e18cf-bucket/pipeline_root/'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin\n",
    "REGION=\"us-east1\"\n",
    "\n",
    "\n",
    "PIPELINE_ROOT = f\"{BUCKET_NAME}/pipeline_root/\"\n",
    "PIPELINE_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8db54bb1-ce09-4b9d-b9aa-51121bc42a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=\"python:3.9\", output_component_file=\"first-component.yaml\")\n",
    "def product_name(text: str) -> str:\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f680d8f0-d830-4055-aebf-6fd900ad2053",
   "metadata": {},
   "source": [
    "The @component decorator compiles this function to a component when the pipeline is run. You'll use this anytime you write a custom component.\n",
    "\n",
    "The base_image parameter specifies the container image this component will use.\n",
    "\n",
    "The output_component_file parameter is optional, and specifies the yaml file to write the compiled component to. After running the cell you should see that file written to your notebook instance. If you wanted to share this component with someone, you could send them the generated yaml file and have them load it with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "434587ad-f0a3-463e-b4a8-3831bb3bbc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_name_component = kfp.components.load_component_from_file('./first-component.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adba29e8-66c9-4aeb-b232-57e42c6b764e",
   "metadata": {},
   "source": [
    "Create two additional components\n",
    "\n",
    "    To complete the pipeline, create two more components. The first one takes a string as input, and converts this string to its corresponding emoji if there is one. It returns a tuple with the input text passed, and the resulting emoji:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22f53303-a7f8-4a64-b29c-805ee393351f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=\"python:3.9\", output_component_file=\"second-component.yaml\", packages_to_install=[\"emoji\"])\n",
    "def emoji(\n",
    "    text: str,\n",
    ") -> NamedTuple(\n",
    "    \"Outputs\",\n",
    "    [\n",
    "        (\"emoji_text\", str),  # Return parameters\n",
    "        (\"emoji\", str),\n",
    "    ],\n",
    "):\n",
    "    import emoji\n",
    "\n",
    "    emoji_text = text\n",
    "    emoji_str = emoji.emojize(':' + emoji_text + ':', language='alias')\n",
    "    print(\"output one: {}; output_two: {}\".format(emoji_text, emoji_str))\n",
    "    return (emoji_text, emoji_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa85a6c-88ca-4cca-8720-721a14b8eda0",
   "metadata": {},
   "source": [
    "This component is a bit more complex than the previous one. Here's what's new:\n",
    "\n",
    "    The packages_to_install parameter tells the component any external library dependencies for this container. In this case, you're using a library called emoji.\n",
    "    This component returns a NamedTuple called Outputs. Notice that each of the strings in this tuple have keys: emoji_text and emoji. You'll use these in your next component to access the output.\n",
    "\n",
    "    The final component in this pipeline will consume the output of the first two and combine them to return a string:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e0513fc-2da2-45b5-87af-c6ae3f8aa854",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=\"python:3.9\", output_component_file=\"third-component.yaml\")\n",
    "def build_sentence(\n",
    "    product: str,\n",
    "    emoji: str,\n",
    "    emojitext: str\n",
    ") -> str:\n",
    "    print(\"We completed the pipeline, hooray!\")\n",
    "    end_str = product + \" is \"\n",
    "    if len(emoji) > 0:\n",
    "        end_str += emoji\n",
    "    else:\n",
    "        end_str += emojitext\n",
    "    return(end_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed10538-3e25-42d7-a2e7-8e609613f360",
   "metadata": {},
   "source": [
    "Putting the components together into a pipeline\n",
    "\n",
    "The component definitions defined above created factory functions that can be used in a pipeline definition to create steps.\n",
    "\n",
    "    To set up a pipeline, use the @dsl.pipeline decorator, give the pipeline a name and description, and provide the root path where your pipeline's artifacts should be written. By artifacts, it means any output files generated by your pipeline. This intro pipeline doesn't generate any, but your next pipeline will.\n",
    "\n",
    "    In the next block of code you define an intro_pipeline function. This is where you specify the inputs to your initial pipeline steps, and how steps connect to each other:\n",
    "\n",
    "    product_task takes a product name as input. Here you're passing \"Vertex Pipelines\" but you can change this to whatever you'd like.\n",
    "    emoji_task takes the text code for an emoji as input. You can also change this to whatever you'd like. For example, \"party_face\" refers to the ğŸ¥³ emoji. Note that since both this and the product_task component don't have any steps that feed input into them, you manually specify the input for these when you define your pipeline.\n",
    "    The last step in the pipeline - consumer_task has three input parameters:\n",
    "        The output of product_task. Since this step only produces one output, you can reference it via product_task.output.\n",
    "        The emoji output of the emoji_task step. See the emoji component defined above where you named the output parameters.\n",
    "        Similarly, the emoji_text named output from the emoji component. In case your pipeline is passed text that doesn't correspond with an emoji, it'll use this text to construct a sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a273eefa-a860-464b-af56-a0c5c77b4ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"hello-world\",\n",
    "    description=\"An intro pipeline\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "\n",
    "# You can change the `text` and `emoji_str` parameters here to update the pipeline output\n",
    "def intro_pipeline(text: str = \"Vertex Pipelines\", emoji_str: str = \"sparkles\"):\n",
    "    product_task = product_name(text)\n",
    "    emoji_task = emoji(emoji_str)\n",
    "    consumer_task = build_sentence(\n",
    "        product_task.output,\n",
    "        emoji_task.outputs[\"emoji\"],\n",
    "        emoji_task.outputs[\"emoji_text\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fab30c5-3dc8-486d-9059-9e8421b1b10e",
   "metadata": {},
   "source": [
    "Compile and run the pipeline\n",
    "\n",
    "    With your pipeline defined, you're ready to compile it. The following will generate a JSON file that you'll use to run the pipeline:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48cdf686-4b67-451f-9a25-24e7daca24ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=intro_pipeline, package_path=\"intro_pipeline_job.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e5fa43-6ee2-4ec9-b12c-6482159e3b14",
   "metadata": {},
   "source": [
    "\n",
    "    Next, instantiate an API client:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b2b7934-d9bf-4f43-8562-acc258c2663f",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_client = AIPlatformClient(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b725e534-a760-4e26-8c23-8465478e12dc",
   "metadata": {},
   "source": [
    "Finally, run the pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d40b6b-f007-43df-9e11-2c3a0f1c6d05",
   "metadata": {},
   "source": [
    "response = api_client.create_run_from_job_spec(\n",
    "    job_spec_path=\"intro_pipeline_job.json\",\n",
    "    # pipeline_root=PIPELINE_ROOT  # this argument is necessary if you did not specify PIPELINE_ROOT as part of the pipeline definition.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42e6bd4-880c-4f7b-8d5b-a5d3a19a29a4",
   "metadata": {},
   "source": [
    "Creating an end-to-end ML pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fe097e-5a87-496d-8e91-4702d64bb472",
   "metadata": {},
   "source": [
    "\n",
    "    Create a Dataset in Vertex AI\n",
    "    Train a tabular classification model with AutoML\n",
    "    Get evaluation metrics on this model\n",
    "    Based on the evaluation metrics, decide whether to deploy the model using conditional logic in Vertex Pipelines\n",
    "    Deploy the model to an endpoint using Vertex Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ba980f-29e0-4abb-bbee-0b587f45c4b0",
   "metadata": {},
   "source": [
    "A custom component for model evaluation\n",
    "\n",
    "The custom component you'll define will be used towards the end of the pipeline once model training has completed. This component will do a few things:\n",
    "\n",
    "    Get the evaluation metrics from the trained AutoML classification model\n",
    "    Parse the metrics and render them in the Vertex Pipelines UI\n",
    "    Compare the metrics to a threshold to determine whether the model should be deployed\n",
    "\n",
    "Before defining the component, understand its input and output parameters. As input, this pipeline takes some metadata on your Cloud project, the resulting trained model (you'll define this component later), the model's evaluation metrics, and a thresholds_dict_str.\n",
    "\n",
    "The thresholds_dict_str is something you'll define when you run your pipeline. In the case of this classification model, this will be the area under the ROC curve value for which you should deploy the model. For example, if you pass in 0.95, that means you'd only like your pipeline to deploy the model if this metric is above 95%.\n",
    "\n",
    "The evaluation component returns a string indicating whether or not to deploy the model.\n",
    "\n",
    "    Add the following in a notebook cell to create this custom component:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac446826-345e-4b42-8e53-0373221a9e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"gcr.io/deeplearning-platform-release/tf2-cpu.2-3:latest\",\n",
    "    output_component_file=\"tables_eval_component.yaml\", # Optional: you can use this to load the component later\n",
    "    packages_to_install=[\"google-cloud-aiplatform\"],\n",
    ")\n",
    "def classif_model_eval_metrics(\n",
    "    project: str,\n",
    "    location: str,  # \"region\",\n",
    "    api_endpoint: str,  # \"region-aiplatform.googleapis.com\",\n",
    "    thresholds_dict_str: str,\n",
    "    model: Input[Model],\n",
    "    metrics: Output[Metrics],\n",
    "    metricsc: Output[ClassificationMetrics],\n",
    ") -> NamedTuple(\"Outputs\", [(\"dep_decision\", str)]):  # Return parameter.\n",
    "\n",
    "    \"\"\"This function renders evaluation metrics for an AutoML Tabular classification model.\n",
    "    It retrieves the classification model evaluation generated by the AutoML Tabular training\n",
    "    process, does some parsing, and uses that info to render the ROC curve and confusion matrix\n",
    "    for the model. It also uses given metrics threshold information and compares that to the\n",
    "    evaluation results to determine whether the model is sufficiently accurate to deploy.\n",
    "    \"\"\"\n",
    "    import json\n",
    "    import logging\n",
    "\n",
    "    from google.cloud import aiplatform\n",
    "\n",
    "    # Fetch model eval info\n",
    "    def get_eval_info(client, model_name):\n",
    "        from google.protobuf.json_format import MessageToDict\n",
    "\n",
    "        response = client.list_model_evaluations(parent=model_name)\n",
    "        metrics_list = []\n",
    "        metrics_string_list = []\n",
    "        for evaluation in response:\n",
    "            print(\"model_evaluation\")\n",
    "            print(\" name:\", evaluation.name)\n",
    "            print(\" metrics_schema_uri:\", evaluation.metrics_schema_uri)\n",
    "            metrics = MessageToDict(evaluation._pb.metrics)\n",
    "            for metric in metrics.keys():\n",
    "                logging.info(\"metric: %s, value: %s\", metric, metrics[metric])\n",
    "            metrics_str = json.dumps(metrics)\n",
    "            metrics_list.append(metrics)\n",
    "            metrics_string_list.append(metrics_str)\n",
    "\n",
    "        return (\n",
    "            evaluation.name,\n",
    "            metrics_list,\n",
    "            metrics_string_list,\n",
    "        )\n",
    "\n",
    "    # Use the given metrics threshold(s) to determine whether the model is\n",
    "    # accurate enough to deploy.\n",
    "    def classification_thresholds_check(metrics_dict, thresholds_dict):\n",
    "        for k, v in thresholds_dict.items():\n",
    "            logging.info(\"k {}, v {}\".format(k, v))\n",
    "            if k in [\"auRoc\", \"auPrc\"]:  # higher is better\n",
    "                if metrics_dict[k] < v:  # if under threshold, don't deploy\n",
    "                    logging.info(\n",
    "                        \"{} < {}; returning False\".format(metrics_dict[k], v)\n",
    "                    )\n",
    "                    return False\n",
    "        logging.info(\"threshold checks passed.\")\n",
    "        return True\n",
    "\n",
    "    def log_metrics(metrics_list, metricsc):\n",
    "        test_confusion_matrix = metrics_list[0][\"confusionMatrix\"]\n",
    "        logging.info(\"rows: %s\", test_confusion_matrix[\"rows\"])\n",
    "\n",
    "        # log the ROC curve\n",
    "        fpr = []\n",
    "        tpr = []\n",
    "        thresholds = []\n",
    "        for item in metrics_list[0][\"confidenceMetrics\"]:\n",
    "            fpr.append(item.get(\"falsePositiveRate\", 0.0))\n",
    "            tpr.append(item.get(\"recall\", 0.0))\n",
    "            thresholds.append(item.get(\"confidenceThreshold\", 0.0))\n",
    "        print(f\"fpr: {fpr}\")\n",
    "        print(f\"tpr: {tpr}\")\n",
    "        print(f\"thresholds: {thresholds}\")\n",
    "        metricsc.log_roc_curve(fpr, tpr, thresholds)\n",
    "\n",
    "        # log the confusion matrix\n",
    "        annotations = []\n",
    "        for item in test_confusion_matrix[\"annotationSpecs\"]:\n",
    "            annotations.append(item[\"displayName\"])\n",
    "        logging.info(\"confusion matrix annotations: %s\", annotations)\n",
    "        metricsc.log_confusion_matrix(\n",
    "            annotations,\n",
    "            test_confusion_matrix[\"rows\"],\n",
    "        )\n",
    "\n",
    "        # log textual metrics info as well\n",
    "        for metric in metrics_list[0].keys():\n",
    "            if metric != \"confidenceMetrics\":\n",
    "                val_string = json.dumps(metrics_list[0][metric])\n",
    "                metrics.log_metric(metric, val_string)\n",
    "        # metrics.metadata[\"model_type\"] = \"AutoML Tabular classification\"\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    aiplatform.init(project=project)\n",
    "    # extract the model resource name from the input Model Artifact\n",
    "    model_resource_path = model.uri.replace(\"aiplatform://v1/\", \"\")\n",
    "    logging.info(\"model path: %s\", model_resource_path)\n",
    "\n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    # Initialize client that will be used to create and send requests.\n",
    "    client = aiplatform.gapic.ModelServiceClient(client_options=client_options)\n",
    "    eval_name, metrics_list, metrics_str_list = get_eval_info(\n",
    "        client, model_resource_path\n",
    "    )\n",
    "    logging.info(\"got evaluation name: %s\", eval_name)\n",
    "    logging.info(\"got metrics list: %s\", metrics_list)\n",
    "    log_metrics(metrics_list, metricsc)\n",
    "\n",
    "    thresholds_dict = json.loads(thresholds_dict_str)\n",
    "    deploy = classification_thresholds_check(metrics_list[0], thresholds_dict)\n",
    "    if deploy:\n",
    "        dep_decision = \"true\"\n",
    "    else:\n",
    "        dep_decision = \"false\"\n",
    "    logging.info(\"deployment decision is %s\", dep_decision)\n",
    "\n",
    "    return (dep_decision,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995013ac-3461-46bb-bd7a-b83e3a6a15bd",
   "metadata": {},
   "source": [
    "Adding Google Cloud pre-built components\n",
    "\n",
    "In this step you'll define the rest of your pipeline components and see how they all fit together.\n",
    "\n",
    "    First, define the display name for your pipeline run using a timestamp:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13547cd6-002a-4da5-89a1-0ea05bf71eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "automl-beans1700398135\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "DISPLAY_NAME = 'automl-beans{}'.format(str(int(time.time())))\n",
    "print(DISPLAY_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6da17e26-146a-4f1e-8f2f-faa5c824e0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(name=\"automl-tab-beans-training-v2\",\n",
    "                  pipeline_root=PIPELINE_ROOT)\n",
    "def pipeline(\n",
    "    bq_source: str = \"bq://aju-dev-demos.beans.beans1\",\n",
    "    display_name: str = DISPLAY_NAME,\n",
    "    project: str = PROJECT_ID,\n",
    "    gcp_region: str = \"us-east1\",\n",
    "    api_endpoint: str = \"us-east1-aiplatform.googleapis.com\",\n",
    "    thresholds_dict_str: str = '{\"auRoc\": 0.95}',\n",
    "):\n",
    "    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n",
    "        project=project, display_name=display_name, bq_source=bq_source\n",
    "    )\n",
    "\n",
    "    training_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n",
    "        project=project,\n",
    "        display_name=display_name,\n",
    "        optimization_prediction_type=\"classification\",\n",
    "        budget_milli_node_hours=1000,\n",
    "        column_transformations=[\n",
    "            {\"numeric\": {\"column_name\": \"Area\"}},\n",
    "            {\"numeric\": {\"column_name\": \"Perimeter\"}},\n",
    "            {\"numeric\": {\"column_name\": \"MajorAxisLength\"}},\n",
    "            {\"numeric\": {\"column_name\": \"MinorAxisLength\"}},\n",
    "            {\"numeric\": {\"column_name\": \"AspectRation\"}},\n",
    "            {\"numeric\": {\"column_name\": \"Eccentricity\"}},\n",
    "            {\"numeric\": {\"column_name\": \"ConvexArea\"}},\n",
    "            {\"numeric\": {\"column_name\": \"EquivDiameter\"}},\n",
    "            {\"numeric\": {\"column_name\": \"Extent\"}},\n",
    "            {\"numeric\": {\"column_name\": \"Solidity\"}},\n",
    "            {\"numeric\": {\"column_name\": \"roundness\"}},\n",
    "            {\"numeric\": {\"column_name\": \"Compactness\"}},\n",
    "            {\"numeric\": {\"column_name\": \"ShapeFactor1\"}},\n",
    "            {\"numeric\": {\"column_name\": \"ShapeFactor2\"}},\n",
    "            {\"numeric\": {\"column_name\": \"ShapeFactor3\"}},\n",
    "            {\"numeric\": {\"column_name\": \"ShapeFactor4\"}},\n",
    "            {\"categorical\": {\"column_name\": \"Class\"}},\n",
    "        ],\n",
    "        dataset=dataset_create_op.outputs[\"dataset\"],\n",
    "        target_column=\"Class\",\n",
    "    )\n",
    "    model_eval_task = classif_model_eval_metrics(\n",
    "        project,\n",
    "        gcp_region,\n",
    "        api_endpoint,\n",
    "        thresholds_dict_str,\n",
    "        training_op.outputs[\"model\"],\n",
    "    )\n",
    "\n",
    "    with dsl.Condition(\n",
    "        model_eval_task.outputs[\"dep_decision\"] == \"true\",\n",
    "        name=\"deploy_decision\",\n",
    "    ):\n",
    "\n",
    "        deploy_op = gcc_aip.ModelDeployOp(  # noqa: F841\n",
    "            model=training_op.outputs[\"model\"],\n",
    "            project=project,\n",
    "            machine_type=\"e2-standard-4\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5550c8e8-003e-4608-ae23-6cf256b29e32",
   "metadata": {},
   "source": [
    "What's happening in this code:\n",
    "\n",
    "    First, just as in the previous pipeline, you define the input parameters this pipeline takes. You need to set these manually since they don't depend on the output of other steps in the pipeline.\n",
    "    The rest of the pipeline uses a few pre-built components for interacting with Vertex AI services:\n",
    "        TabularDatasetCreateOp creates a tabular dataset in Vertex AI given a dataset source either in Cloud Storage or BigQuery. In this pipeline, you're passing the data via a BigQuery table URL.\n",
    "        AutoMLTabularTrainingJobRunOp kicks off an AutoML training job for a tabular dataset. You pass a few configuration parameters to this component, including the model type (in this case, classification), some data on the columns, how long you'd like to run training for, and a pointer to the dataset. Notice that to pass in the dataset to this component, you're providing the output of the previous component via dataset_create_op.outputs[\"dataset\"] .\n",
    "        ModelDeployOp deploys a given model to an endpoint in Vertex AI. There are additional configuration options available, but here you're providing the endpoint machine type, project, and model you'd like to deploy. You're passing in the model by accessing the outputs of the training step in your pipeline.\n",
    "    This pipeline also makes use of conditional logic, a feature of Vertex Pipelines that lets you define a condition, along with different branches based on the result of that condition. Remember that when you defined the pipeline you passed a thresholds_dict_str parameter. This is the accuracy threshold you're using to determine whether to deploy your model to an endpoint. To implement this, make use of the Condition class from the KFP SDK. The condition passed in is the output of the custom eval component you defined earlier in this lab. If this condition is true, the pipeline will continue to execute the deploy_op component. If accuracy doesn't meet the predefined threshold, the pipeline will stop here and won't deploy a model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe5c4ca-6526-4123-9905-dae5d9a2ab1d",
   "metadata": {},
   "source": [
    "Compile and run the end-to-end ML pipeline\n",
    "\n",
    "    With the full pipeline defined, it's time to compile it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d9d0320-7751-48e3-9802-7ed8d0f6047c",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"tab_classif_pipeline.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f61c114c-cb94-4fb2-bb20-f43837b4644d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:googleapiclient.discovery:URL being requested: POST https://us-east1-aiplatform.googleapis.com/v1beta1/projects/qwiklabs-gcp-01-8b55c97e18cf/locations/us-east1/pipelineJobs?pipelineJobId=automl-tab-beans-training-v2-20231119125147&alt=json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-east1/pipelines/runs/automl-tab-beans-training-v2-20231119125147?project=qwiklabs-gcp-01-8b55c97e18cf\" target=\"_blank\" >here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = api_client.create_run_from_job_spec(\n",
    "    \"tab_classif_pipeline.json\", pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={\"project\": PROJECT_ID,\"display_name\": DISPLAY_NAME}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9b3ec2-eeee-4d75-b877-feba63a38c1a",
   "metadata": {},
   "source": [
    "If you toggle the \"Expand artifacts\" button at the top, you'll be able to see details for the different artifacts created from your pipeline. For example, if you click on the dataset artifact, you'll see details on the Vertex AI dataset that was created. You can click the link here to go to the page for that dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bf71ed-4f3e-4f9d-bb6a-6d2ec4cc9da7",
   "metadata": {},
   "source": [
    "Similarly, to see the resulting metric visualizations from your custom evaluation component, click on the artifact called metricsc. On the right side of your dashboard, you'll be able to see the confusion matrix for this model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebf2aa0-dea0-4027-8823-a813f88e1061",
   "metadata": {},
   "source": [
    "To see the model and endpoint created from this pipeline run, go to the models section and click on the model named automl-beans. There you should see this model deployed to an endpoint:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d934306-9ec4-4e72-9bc6-ca96532bd2ff",
   "metadata": {},
   "source": [
    "Comparing metrics across pipeline runs\n",
    "\n",
    "    If you run this pipeline multiple times, you may want to compare metrics across runs. You can use the aiplatform.get_pipeline_df() method to access run metadata. Here, we'll get metadata for all runs of this pipeline and load it into a Pandas DataFrame:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb42c504-c0c2-479d-afef-d1cd4ec40f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_df = aiplatform.get_pipeline_df(pipeline=\"automl-tab-beans-training-v2\")\n",
    "small_pipeline_df = pipeline_df.head(2)\n",
    "small_pipeline_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55356f08-ac13-42fb-81bd-bc2fcc0b3e54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
